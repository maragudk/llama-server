# llama-server

A simple layer on top of `ghcr.io/ggml-org/llama.cpp:server` to load GGUF models from `assets.maragu.dev` at runtime.
